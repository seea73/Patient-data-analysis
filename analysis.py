# -*- coding: utf-8 -*-
"""PD_asg3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IH7HX4jzXGCz8GXvskWce7Kb7AW39D8q
"""

!pip install seaborn scikit-learn

#Import Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.metrics import classification_report
import seaborn as sns

# Upload the CSV manually
from google.colab import files
uploaded = files.upload()

# Load the dataset
import pandas as pd
df = pd.read_csv(next(iter(uploaded)))
df.head()

# Initial Data Checks
print("Duplicate Rows:", df.duplicated().sum())
print("Missing Values:\n", df.isnull().sum())
print("Unique Values per Column:\n", df.nunique())

#Rename columns for ease if needed
df.columns = [col.strip().upper().replace(" ", "_") for col in df.columns]

#1
sns.histplot(df['AGE'], kde=True, bins=30, color='skyblue')
plt.title('Age Distribution of Patients')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

#2
sns.countplot(data=df, x='SEX', palette='pastel')
plt.title('Gender Distribution')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.show()

#3
lab_features = ['HAEMATOCRIT', 'HAEMOGLOBINS', 'ERYTHROCYTE', 'LEUCOCYTE', 'THROMBOCYTE', 'MCH', 'MCHC', 'MCV']
for feature in lab_features:
    sns.histplot(df[feature], kde=True)
    plt.title(f'Distribution of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Frequency')
    plt.show()

#4
numeric_df = df.select_dtypes(include='number')
plt.figure(figsize=(10, 8))
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

#5
for feature in lab_features:
    sns.boxenplot(data=df, x='SOURCE', y=feature, palette='Set2')
    plt.title(f'{feature} vs Patient Care Type')
    plt.xlabel('SOURCE (0 = Out-care, 1 = In-care)')
    plt.ylabel(feature)
    plt.show()

#6
# Create HAEM flag based on sex-specific HAEMATOCRIT thresholds
df["HAEM"] = np.nan
df.loc[df.SEX == "M", "HAEM"] = ((df.HAEMATOCRIT >= 40.7) & (df.HAEMATOCRIT <= 50.3))
df.loc[df.SEX == "F", "HAEM"] = ((df.HAEMATOCRIT >= 36.1) & (df.HAEMATOCRIT <= 44.3))

# Countplot of HAEM by gender
sns.countplot(data=df, x='SEX', hue='HAEM', palette='cool')
plt.title('Normal HAEMATOCRIT by Gender')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.legend(title='Normal Range')
plt.show()

#7
# Create RATIO column
df['RATIO'] = df['ERYTHROCYTE'] / df['LEUCOCYTE']

sns.boxplot(data=df, x='SOURCE', y='RATIO', palette='Accent')
plt.title('Erythrocyte/Leucocyte Ratio by Patient Class')
plt.xlabel('SOURCE (0 = Out-care, 1 = In-care)')
plt.ylabel('RATIO')
plt.show()

#8

from sklearn.model_selection import train_test_split
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.metrics import classification_report

# Preprocessing
df['SEX_ENCODED'] = df['SEX'].map({'M': 0, 'F': 1})
features = ['AGE', 'SEX_ENCODED'] + lab_features + ['RATIO']
X = df[features]
y = df['SOURCE']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)

# Model training
model = HistGradientBoostingClassifier(random_state=42)
model.fit(X_train, y_train)

# Evaluation
y_pred = model.predict(X_test)
print("Classification Report (HistGradientBoosting):\n", classification_report(y_test, y_pred))

#9
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score

models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(random_state=42),
    'HistGradientBoosting': HistGradientBoostingClassifier(random_state=42)
}

for name, clf in models.items():
    clf.fit(X_train, y_train)
    pred = clf.predict(X_test)
    score = f1_score(y_test, pred, average='macro')
    print(f'{name} F1 Score: {score:.4f}')

#10
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)

importances = pd.Series(rf.feature_importances_, index=features)
importances.sort_values().plot(kind='barh', figsize=(8, 6), color='teal')
plt.title('Feature Importances from Random Forest')
plt.xlabel('Importance')
plt.show()